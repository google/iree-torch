{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0,\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"provenance\": [],\n",
    "      \"collapsed_sections\": [],\n",
    "      \"include_colab_link\": true\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    }\n",
    "  },\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"view-in-github\",\n",
    "        \"colab_type\": \"text\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"<a href=\\\"https://colab.research.google.com/gist/dellis23/9816c333a80ca0b5aa9a271e2d63a157/iree-torch-bert-test.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# Introduction\\n\",\n",
    "        \"\\n\",\n",
    "        \"In this notebook, we will take a PyTorch Hugging Face BERT model and compile it down to a format executable by IREE.  We will then demonstrate the significantly reduced runtime size.  Additional features of IREE can be found on the [IREE homepage](https://iree-org.github.io/iree/#key-features).\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ZvZ62ewxpi9L\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# Package Installation\\n\",\n",
    "        \"\\n\",\n",
    "        \"To install `torch-mlir` (required to compile the model to a format processable by IREE), your Python version must be 3.9 or 3.10.\\n\",\n",
    "        \"\\n\",\n",
    "        \"As of September 2022, Colab only runs on 3.7.  You must use a [local Colab runtime](https://research.google.com/colaboratory/local-runtimes.html) with the correct Python version for this notebook to work correctly.\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"df1pAY3poNEq\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"import platform\\n\",\n",
    "        \"assert platform.python_version().startswith('3.9.') or platform.python_version().startswith('3.10.')\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"yHFrHZ1DfrT0\"\n",
    "      },\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"%%capture\\n\",\n",
    "        \"!pip install -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html torch==1.13.0.dev20220913+cpu\\n\",\n",
    "        \"!pip install https://github.com/llvm/torch-mlir/releases/download/snapshot-20220913.595/torch_mlir-20220913.595-cp310-cp310-linux_x86_64.whl\\n\",\n",
    "        \"!pip install iree-compiler iree-runtime iree-tools-tf -f https://github.com/iree-org/iree/releases\\n\",\n",
    "        \"!pip install git+https://github.com/iree-org/iree-torch.git\\n\",\n",
    "        \"!pip install transformers\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"-v_vEvT8V1La\"\n",
    "      },\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# Model Setup\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"a9iMOfulobtY\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## TODO: Give a brief explanation of why this wrapping is necessary.\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"iaTqNHjpoyWl\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"a9i_RnThVpxU\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import torch\\n\",\n",
    "        \"import torch_mlir\\n\",\n",
    "        \"import iree_torch\\n\",\n",
    "        \"\\n\",\n",
    "        \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def prepare_sentence_tokens(hf_model: str, sentence: str):\\n\",\n",
    "        \"    tokenizer = AutoTokenizer.from_pretrained(hf_model)\\n\",\n",
    "        \"    return torch.tensor([tokenizer.encode(sentence)])\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class OnlyLogitsHuggingFaceModel(torch.nn.Module):\\n\",\n",
    "        \"    \\\"\\\"\\\"Wrapper that returns only the logits from a HuggingFace model.\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def __init__(self, model_name: str):\\n\",\n",
    "        \"        super().__init__()\\n\",\n",
    "        \"        self.model = AutoModelForSequenceClassification.from_pretrained(\\n\",\n",
    "        \"            model_name,  # The pretrained model name.\\n\",\n",
    "        \"            # The number of output labels--2 for binary classification.\\n\",\n",
    "        \"            num_labels=2,\\n\",\n",
    "        \"            # Whether the model returns attentions weights.\\n\",\n",
    "        \"            output_attentions=False,\\n\",\n",
    "        \"            # Whether the model returns all hidden-states.\\n\",\n",
    "        \"            output_hidden_states=False,\\n\",\n",
    "        \"            torchscript=True,\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        self.model.eval()\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def forward(self, input):\\n\",\n",
    "        \"        # Return only the logits.\\n\",\n",
    "        \"        return self.model(input)[0]\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Suppress warnings\\n\",\n",
    "        \"import warnings\\n\",\n",
    "        \"warnings.simplefilter(\\\"ignore\\\")\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"os.environ[\\\"TOKENIZERS_PARALLELISM\\\"] = \\\"true\\\"\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# IREE Compilation\\n\",\n",
    "        \"\\n\",\n",
    "        \"Now, our PyTorch model can be compiled to MLIR and then to a format IREE is able to load and execute.\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ENzdaB8zogBj\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"# The HuggingFace model name to use\\n\",\n",
    "        \"model_name = \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# The sentence to run the model on\\n\",\n",
    "        \"sentence = \\\"The quick brown fox jumps over the lazy dog.\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Parsing sentence tokens.\\\")\\n\",\n",
    "        \"example_input = prepare_sentence_tokens(model_name, sentence)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Instantiating model.\\\")\\n\",\n",
    "        \"model = OnlyLogitsHuggingFaceModel(model_name)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Tracing model.\\\")\\n\",\n",
    "        \"traced = torch.jit.trace(model, example_input)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Compiling with Torch-MLIR\\\")\\n\",\n",
    "        \"linalg_on_tensors_mlir = torch_mlir.compile(\\n\",\n",
    "        \"    traced,\\n\",\n",
    "        \"    example_input,\\n\",\n",
    "        \"    output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Compiling with IREE\\\")\\n\",\n",
    "        \"# Backend options:\\n\",\n",
    "        \"#\\n\",\n",
    "        \"# llvm-cpu - cpu, native code\\n\",\n",
    "        \"# vmvx - cpu, interpreted\\n\",\n",
    "        \"# vulkan - GPU for general GPU devices\\n\",\n",
    "        \"# cuda - GPU for NVIDIA devices\\n\",\n",
    "        \"iree_backend = \\\"llvm-cpu\\\"\\n\",\n",
    "        \"iree_vmfb = iree_torch.compile_to_vmfb(linalg_on_tensors_mlir, iree_backend)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Loading in IREE\\\")\\n\",\n",
    "        \"invoker = iree_torch.load_vmfb(iree_vmfb, iree_backend)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Running on IREE\\\")\\n\",\n",
    "        \"result = invoker.forward(example_input)\\n\",\n",
    "        \"print(\\\"RESULT:\\\", result)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"b6PqMZfXidbq\",\n",
    "        \"outputId\": \"65404058-bd73-4401-b5cf-cfc1acbe9f67\"\n",
    "      },\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Parsing sentence tokens.\\n\",\n",
    "            \"Instantiating model.\\n\",\n",
    "            \"Tracing model.\\n\",\n",
    "            \"Compiling with Torch-MLIR\\n\",\n",
    "            \"Compiling with IREE\\n\",\n",
    "            \"Loading in IREE\\n\",\n",
    "            \"Running on IREE\\n\",\n",
    "            \"RESULT: tensor([[ 1.8574, -1.8036]])\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"We are now running our model on IREE.  The compiled version of this model can be saved, deployed, and executed independently of PyTorch.\\n\",\n",
    "        \"\\n\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"x3njgRYXnacc\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# Runtime Size Comparison\\n\",\n",
    "        \"\\n\",\n",
    "        \"One benefit of running a model on IREE is lightweight deployment.  The IREE runtime has a significantly smaller footprint than a full PyTorch install.\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"zhrwdh9pmPIH\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"import os\\n\",\n",
    "        \"!du -sh {os.path.dirname(torch.__file__)}\\n\",\n",
    "        \"import iree.runtime as iree_runtime\\n\",\n",
    "        \"!du -sh {os.path.dirname(iree_runtime.__file__)}\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"Ueyx5zvUly7M\",\n",
    "        \"outputId\": \"61440bd9-d166-46d7-b1f6-ee922a61c355\"\n",
    "      },\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"713M\\t/usr/local/google/home/danielellis/colab-test-venv/lib/python3.10/site-packages/torch\\n\",\n",
    "            \"4.0M\\t/usr/local/google/home/danielellis/colab-test-venv/lib/python3.10/site-packages/iree/runtime\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
